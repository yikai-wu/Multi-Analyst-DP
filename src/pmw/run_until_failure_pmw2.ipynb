{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f52b58df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from matplotlib import pyplot as plt\n",
    "from collections import OrderedDict\n",
    "from src.hdmm.workload import AllRange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cec2d91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.float_format', '{:0.2f}'.format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64eae0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getBounds(df):\n",
    "    \"\"\"\n",
    "    Returns [upper bound error, lower bound error]\n",
    "    \"\"\"\n",
    "    return [df.abs_error.min(), df.abs_error.max()]\n",
    "\n",
    "def getAverageError(df):\n",
    "    \"\"\"\n",
    "    Returns average error across all queries\n",
    "    \"\"\"\n",
    "    return df.abs_error.sum() / len(df)\n",
    "\n",
    "def printBoundsAndAvgError(df):\n",
    "    print(f'Average error is {getAverageError(df)}. Lower bound is {getBounds(df)[0]} and upper bound is {getBounds(df)[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77bd3f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pmw2(workload, x, T, eps=0.01, k=0, show_messages=True, to_return='pd', analyst_labels = [],\n",
    "       show_plot=False, show_failure_step=True):\n",
    "    \"\"\"\n",
    "    Implement Private Multiplicative Weights Mechanism (PMW) on a workload of\n",
    "    linear queries. New arguments to allow for optimizing the amount of\n",
    "    privacy budget used in each step.\n",
    "    \n",
    "    to_return argument determines what the function will return. \n",
    "        - if 'pd', pmw() returns pandas df with test data for each \n",
    "        query (query, d_t_hat, updated, algo_ans, real_ans, abs_error, \n",
    "        rel_error). \n",
    "        - if 'update_count', pmw() returns the update count for the total\n",
    "        amount of queries\n",
    "\n",
    "    - W = workload of queries (M x k numpy array)\n",
    "    - x = true database (M x 1 numpy array)\n",
    "    - eps = privacy budget\n",
    "    - k = number of update steps\n",
    "    - T  = update threshold\n",
    "    \"\"\" \n",
    "    \n",
    "    # initialize constants\n",
    "    m = x.size  # database len\n",
    "    n = x.sum()\n",
    "    eta = (math.log(m, np.e) ** (1 / 4)) / (math.sqrt(n))\n",
    "    # Set k to be the desired number of update rounds\n",
    "    if k==0: # essentially, if k hasn't been changed from its default value, use the length of the workload\n",
    "        k = len(workload)  # num of queries\n",
    "    delta = 1 / (n * math.log(n, np.e))\n",
    "    x_norm = x / np.sum(x)\n",
    "    # synthetic databases at time 0 (prior to any queries)\n",
    "    y_t = np.ones(m) / m\n",
    "    x_t = np.ones(m) / m\n",
    "\n",
    "    # append to list of databases y_t and x_t\n",
    "    x_list = [x_t]\n",
    "    \n",
    "    update_list = []\n",
    "    update_count = 0\n",
    "    pmw_answers = []\n",
    "    update_times = []\n",
    "    d_t_hat_list = []\n",
    "    \n",
    "    failure_mode = False # if reaches failure, failure mode because true and only runs lazy rounds\n",
    "    \n",
    "    def lazy_round():\n",
    "        update_list.append('no')\n",
    "        pmw_answers.append(np.dot(query, x_list[time]))\n",
    "        x_list.append(x_list[time].round(3))\n",
    "    \n",
    "    #inititate first instance of SVT with half the budget and k updates\n",
    "    #will be reset in the main loop\n",
    "    SVTtrigger = False \n",
    "    SVTepsilon1 =((eps/2)/2)\n",
    "    SVTepsilon2 = ((eps/2)/2)\n",
    "    rho = np.random.laplace(loc=0, scale=(1/SVTepsilon1), size=1)[0]\n",
    "    # iterate through time \n",
    "    for time, query in enumerate(workload):\n",
    "        # Do one round of sparce vector technique \n",
    "        \n",
    "        # compute noisy answer by adding Laplacian noise\n",
    "        a_t = np.random.laplace(loc=0, scale=(2*k/SVTepsilon2), size=1)[0]\n",
    "        a_t_hat = (np.dot(query, x_norm)*n ) + a_t\n",
    "\n",
    "        # difference between noisy and maintained histogram answer\n",
    "        d_t_hat = a_t_hat - (n*np.dot(query, x_list[time]))\n",
    "        \n",
    "        # lazy round: use maintained histogram to answer the query\n",
    "        if (abs(d_t_hat) <= T + rho) or failure_mode:\n",
    "            d_t_hat_list.append(d_t_hat)\n",
    "            lazy_round()\n",
    "            continue\n",
    "\n",
    "        # update round: update histogram and return noisy answer\n",
    "        else:\n",
    "            update_times.append(time)\n",
    "            #make a new noisy query answer using some of the leftover budget\n",
    "            a_t = np.random.laplace(loc=0, scale=(2*k/eps), size=1)[0]\n",
    "            a_t_hat = (np.dot(query, x_norm)*n ) + a_t\n",
    "            d_t_hat = a_t_hat - (n*np.dot(query, x_list[time]))\n",
    "            d_t_hat_list.append(d_t_hat)\n",
    "            \n",
    "            # step a\n",
    "            if d_t_hat < 0:\n",
    "                r_t = query\n",
    "            else:\n",
    "                r_t = np.ones(m) - query\n",
    "            for i, v in enumerate(y_t):\n",
    "                #y_t[i] = x_list[time][i] * math.exp(-eta * r_t[i])\n",
    "                y_t[i] = x_list[time][i] * math.exp((d_t_hat/(2*n)) * query[i])\n",
    "            \n",
    "            #print(d_t_hat)\n",
    "            #print(query)\n",
    "            #print(x_list[time])\n",
    "            # step b\n",
    "            x_t = y_t / np.sum(y_t)\n",
    "            update_count = update_list.count('yes')\n",
    "            \n",
    "            if update_count >= k: # threshold for num updates is reached, enter failure_mode\n",
    "                failure_mode = True\n",
    "                if show_failure_step:\n",
    "                    print(f'Failure mode reached at query number {time}: {query}')\n",
    "                lazy_round()\n",
    "            else: # threshold for num updates is not reached yet\n",
    "                x_list.append(x_t.round(3))\n",
    "                update_list.append('yes') # increment number of updates counter\n",
    "                pmw_answers.append(a_t_hat / np.sum(x))\n",
    "                \n",
    "    update_count = update_list.count('yes')      \n",
    "\n",
    "    # calculate error\n",
    "    real_ans = np.matmul(workload, x_norm)\n",
    "    abs_error = np.abs(pmw_answers - real_ans)\n",
    "    rel_error = np.abs(abs_error / np.where(real_ans == 0, 0.000001,\n",
    "                                                real_ans))\n",
    "    \n",
    "    def print_outputs():\n",
    "        np.set_printoptions(suppress=True)\n",
    "        \"\"\"Print inputes/outputs to analyze each query\"\"\"\n",
    "        print(f'Original database: {x}\\n')\n",
    "        print(f'Normalized database: {x_norm}\\n')\n",
    "        print(f'Updated Database = {x_t}\\n')\n",
    "        print(f'Update Count = {update_count}\\n')\n",
    "        print(f'{T=}\\n')\n",
    "        print(f'Error Scale Query Answer= {2*((2*k/eps)**2)}\\n')\n",
    "        print(f'Error Scale SVT= {2*((2*k/SVTepsilon2)**2)}\\n')\n",
    "        print(f'Update Parameter Scale = {eta}\\n')\n",
    "        print(f'{delta=}\\n')\n",
    "    \n",
    "    if show_messages:\n",
    "        print_outputs()\n",
    "        \n",
    "    if show_plot: \n",
    "        plt.xticks(range(0, len(workload), 5))\n",
    "        plt.title('Error across queries:')\n",
    "        #rel_line, = plt.plot(rel_error, label='Relative Error')\n",
    "        abs_line, = plt.plot(abs_error, label='Absolute Error')\n",
    "        for xc in update_times:\n",
    "            plt.axvline(x=xc, color='red', label='Update Times', linestyle='dashed')\n",
    "        plt.legend(handles=[abs_line]) #,rel_line])\n",
    "        \n",
    "    if to_return == \"pd\":\n",
    "        x_list.pop(0).tolist() # remove the first synthetic database to keep length of lists consistent-x_list[t] represents the synthetic database at the end of time t\n",
    "        \n",
    "        \n",
    "        d = {\n",
    "            'algo_ans': pmw_answers,\n",
    "            'real_ans': real_ans.tolist(),\n",
    "            'queries': workload.tolist(), \n",
    "            'updated': update_list,\n",
    "            'abs_error': abs_error,               \n",
    "#            'rel_error': rel_error,\n",
    "            'synthetic database': x_list,\n",
    "            'analyst': analyst_labels,\n",
    "            'd_t_hat': d_t_hat_list, \n",
    "\n",
    "             }\n",
    "        test_data = pd.DataFrame(data=d)\n",
    "        test_data = test_data.round(3)\n",
    "        return test_data\n",
    "    \n",
    "    if to_return == \"error\":\n",
    "        #unique_analysts = list(OrderedDict.fromkeys(analyst_labels))\n",
    "        \n",
    "        d = {'analyst': analyst_labels,\n",
    "             'abs_error': abs_error,               \n",
    "             'rel_error': rel_error,}\n",
    "        data = pd.DataFrame(data=d)\n",
    "        data = data.round(3)\n",
    "        \n",
    "        analyst_error = {}\n",
    "        for analyst in list(set(analyst_labels)):\n",
    "            analyst_error[analyst] = data[data.analyst==analyst]['abs_error'].sum()\n",
    "        return analyst_error\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea17ca88",
   "metadata": {},
   "source": [
    "### Initializing workloads and databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c2a073ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_small = np.array([20, 160, 20, 20, 20, 160, 20, 20])\n",
    "normalized = x_small / x_small.sum()\n",
    "m = x_small.size  # database len\n",
    "n = x_small.sum()\n",
    "#print(f'the threshold for failure is {n * math.log(m, np.e) ** (1 / 2)}')\n",
    "\n",
    "random_array = np.random.randint(2, size=(500,4))\n",
    "zero_array = np.zeros((500,4))\n",
    "alice = np.hstack((random_array, zero_array))\n",
    "bob = np.hstack((zero_array, random_array))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "150ca2fd",
   "metadata": {},
   "source": [
    "We've shown that Alice can use up all the privacy budget. \n",
    "\n",
    "Next, let's take some of the basic adaptations of pmw and see if we can get the same effects to happen or a similar effect were people get similar results indpenedently\n",
    "\n",
    "### To Dos: \n",
    "\n",
    "Instead of having a communal pool of update steps we can do, split the amount of update steps evenly across all analysts. If your update steps are not a multiple of the analysts, you can change the number of update steps.  \n",
    "\n",
    "Look for the same kind of violation where either Bob or Alice will have more overall error in the joint case as opposed to the independent case.\n",
    "\n",
    "In the independent state, you can use the existing pmw algorihtm.\n",
    "\n",
    "Suggestion: When you split it to their individual settings, give them the same amount of update steps that they would've gotten in the group. \n",
    "\n",
    "Individually - 1 ep, 5 k. together - 2 ep, 10k. \n",
    "\n",
    "### Experiments: \n",
    "Try original pmw, adapted w equal update steps pmw, individual: \n",
    "1. [done] A and B query disjoint sections\n",
    "2. [done] A queries the entire dataset except for last index. B queries entire dataset, last index inclusive. \n",
    "3. [done] A asks singleton, B asks all range queries\n",
    "4. [if time] Is total error the right metric? Context: You might run into a case where you get flat error across all of your queries. You may get a lot worse error on one specific query, but the overall error is better. Now Alice and Bob care about the same data. Alice eats all the budget (entire database except for last). Bob cares about entire database, but his queries about the last index has higher error. \n",
    "    \n",
    "    \n",
    "Scenarios to look for: \n",
    "1. Regular pmw performs poorly, but adapted pmw works well. (probably in disjoint setting)\n",
    "2. Scenarios where regular AND adapted pmw perform poorly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36962c66",
   "metadata": {},
   "source": [
    "## Scenario 1: Disjoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ced25535",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alice_first</th>\n",
       "      <th>individual</th>\n",
       "      <th>bob_first</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Alice</th>\n",
       "      <td>1.72</td>\n",
       "      <td>2.60</td>\n",
       "      <td>3.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bob</th>\n",
       "      <td>3.41</td>\n",
       "      <td>2.60</td>\n",
       "      <td>1.72</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       alice_first  individual  bob_first\n",
       "Alice         1.72        2.60       3.41\n",
       "Bob           3.41        2.60       1.72"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# initialize\n",
    "s1random_array = np.random.randint(2, size=(25,4))\n",
    "s1zero_array = np.zeros((25,4))\n",
    "s1alice_q = np.hstack((s1random_array, s1zero_array))\n",
    "s1bob_q = np.hstack((s1zero_array, s1random_array))\n",
    "\n",
    "s1combined_list = []\n",
    "s1bobfirst_list = []\n",
    "s1individual_list = []\n",
    "\n",
    "for i in range(1000):\n",
    "    # combined\n",
    "    s1combined = pmw2(workload=np.vstack((s1alice_q, s1bob_q)), x=x_small, eps=2, T=40, k=10,  \n",
    "                                 analyst_labels=['Alice'] * 25 + ['Bob'] * 25, \n",
    "                                 to_return='error', show_plot=False, show_messages=False, show_failure_step=False)\n",
    "    s1combined_list.append(s1combined)\n",
    "    \n",
    "    s1bobfirst = pmw2(workload=np.vstack((s1bob_q, s1alice_q)), x=x_small, eps=2, T=40, k=10,  \n",
    "                                 analyst_labels=['Bob'] * 25 + ['Alice'] * 25, \n",
    "                                 to_return='error', show_plot=False, show_messages=False, show_failure_step=False)\n",
    "    s1bobfirst_list.append(s1bobfirst)\n",
    "    \n",
    "    # individual\n",
    "    s1a = pmw2(workload=s1alice_q, x=x_small, eps=1, T=40, k=5,\n",
    "               analyst_labels=['Alice'] * 25, \n",
    "               to_return='error', show_plot=False, show_messages=False, show_failure_step=False)\n",
    "    s1b = pmw2(workload=s1bob_q, x=x_small, eps=1, T=40, k=5,\n",
    "               analyst_labels=['Bob'] * 25, \n",
    "               to_return='error', show_plot=False, show_messages=False, show_failure_step=False)\n",
    "    s1individual = {**s1a, **s1b}\n",
    "    s1individual_list.append(s1individual)\n",
    "    \n",
    "# find mean over multiple trials\n",
    "s1combined_average = dict(pd.DataFrame(s1combined_list).mean())\n",
    "s1bobfirst_average = dict(pd.DataFrame(s1bobfirst_list).mean())\n",
    "s1individual_average = dict(pd.DataFrame(s1individual_list).mean())\n",
    "\n",
    "d = {'alice_first': s1combined_average, 'individual': s1individual_average, 'bob_first': s1bobfirst_average}\n",
    "df = pd.DataFrame(data=d).sort_index()\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36abc58",
   "metadata": {},
   "source": [
    "Bob always receives more error in the combined setting than in the individual setting.\n",
    "\n",
    "Questions: \n",
    "\n",
    "1. Why does Bob always receives more error than Alice in the individual setting if their queries are not related? I would expect instead that their individual error would be very similar. **Bob's spike is larger**\n",
    "    \n",
    "2. Alice receives substantially more error in the individual setting than the combined setting. Is this supposed to happen? Although she receives less update steps in the individual setting, shouldn't the smaller epsilon value in the inidivual case counteract this effect? **Alice gets more update steps in the combined setting (eats Bob's)**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8b47e834",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average error is 0.10342000000000001. Lower bound is 0.0 and upper bound is 0.245\n"
     ]
    }
   ],
   "source": [
    "printBoundsAndAvgError(pmw2(workload=np.vstack((s1alice_q, s1bob_q)), x=x_small, eps=2, T=40, k=10,  \n",
    "                                 analyst_labels=['Alice'] * 25 + ['Bob'] * 25, \n",
    "                                 to_return='pd', show_plot=False, show_messages=False, show_failure_step=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ac519d",
   "metadata": {},
   "source": [
    "We measure average error becauase we want to think about infinite query sequences - the types of sequences that PMW are very good at. Average error is better for longer query sequences. PMW is designed to make guarantees on average error. The error will be less than $\\alpha$ with some probability of failure $\\beta$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d52e699",
   "metadata": {},
   "source": [
    "# Scenario 2: Last Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e02ad865",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alice_first</th>\n",
       "      <th>individual</th>\n",
       "      <th>bob_first</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Alice</th>\n",
       "      <td>1.90</td>\n",
       "      <td>2.82</td>\n",
       "      <td>3.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bob</th>\n",
       "      <td>3.19</td>\n",
       "      <td>2.84</td>\n",
       "      <td>1.72</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       alice_first  individual  bob_first\n",
       "Alice         1.90        2.82       3.41\n",
       "Bob           3.19        2.84       1.72"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "s2combined_list = []\n",
    "s2individual_list = []\n",
    "s2bobfirst_list = []\n",
    "\n",
    "for i in range(1000):\n",
    "    # combined\n",
    "    s2alice_q = np.hstack((np.random.randint(2, size=(25,7)), np.zeros((25,1))))\n",
    "    s2bob_q = np.random.randint(2, size=(25,8))\n",
    "\n",
    "    s2combined = pmw2(workload=np.vstack((s2alice_q, s2bob_q)), x=x_small, eps=2, T=40, k=10,  \n",
    "                                 analyst_labels=['Alice'] * 25 + ['Bob'] * 25, \n",
    "                                 to_return='error', show_plot=False, show_messages=False, show_failure_step=False)\n",
    "    s2combined_list.append(s2combined)\n",
    "\n",
    "    \n",
    "    s2bobfirst = pmw2(workload=np.vstack((s2bob_q, s2alice_q)), x=x_small, eps=2, T=40, k=10,  \n",
    "                                 analyst_labels= ['Bob'] * 25 + ['Alice'] * 25, \n",
    "                                 to_return='error', show_plot=False, show_messages=False, show_failure_step=False)\n",
    "    s2bobfirst_list.append(s1bobfirst)\n",
    "    \n",
    "    # individual\n",
    "    s2a = pmw2(workload=s2alice_q, x=x_small, eps=1, T=40, k=5,\n",
    "               analyst_labels=['Alice'] * 25, \n",
    "               to_return='error', show_plot=False, show_messages=False, show_failure_step=False)\n",
    "    s2b = pmw2(workload=s2bob_q, x=x_small, eps=1, T=40, k=5,\n",
    "               analyst_labels=['Bob'] * 25, \n",
    "               to_return='error', show_plot=False, show_messages=False, show_failure_step=False)\n",
    "    s2individual = {**s2a, **s2b}\n",
    "    s2individual_list.append(s2individual)\n",
    "\n",
    "# find mean over multiple trials\n",
    "s2combined_average = dict(pd.DataFrame(s2combined_list).mean())\n",
    "s2individual_average = dict(pd.DataFrame(s2individual_list).mean())\n",
    "s2bobfirst_average = dict(pd.DataFrame(s1bobfirst_list).mean())\n",
    "\n",
    "d = {'alice_first': s2combined_average, 'individual': s2individual_average, 'bob_first': s2bobfirst_average}\n",
    "df = pd.DataFrame(data=d).sort_index()\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "924f9750",
   "metadata": {},
   "source": [
    "all indices except last -> all indices: 1.29 difference\n",
    "all indices -> all indices except last: 1.69 difference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3632f973",
   "metadata": {},
   "source": [
    "Alice is doing better than Bob in the individual and combined setting. \n",
    "\n",
    "When Bob goes first, he suffers less error than alice did when she went first. \n",
    "\n",
    "**Alice's error** in `bob_first` > **Bob's error** in `alice_first`. This makes the empirical point that if the second person queries less from the dataset than the first person, they face more error than going second if you query more from the dataset than the first person. This doesn't make sense to me. Shouldn't the second person that explores more of the dataset that has been previously unexplored experience more error than the second person in the other scenario that explores less of the dataset that had already been previous explored?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b070f1fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average error is 0.09866000000000001. Lower bound is 0.0 and upper bound is 0.329\n"
     ]
    }
   ],
   "source": [
    "printBoundsAndAvgError(pmw2(workload=np.vstack((s2alice_q, s2bob_q)), x=x_small, eps=2, T=40, k=10,  \n",
    "                                 analyst_labels=['Alice'] * 25 + ['Bob'] * 25, \n",
    "                                 to_return='pd', show_plot=False, show_messages=False, show_failure_step=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04abce05",
   "metadata": {},
   "source": [
    "# Scenario 3: Incompatible Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4d3a6d9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alice_first</th>\n",
       "      <th>individual</th>\n",
       "      <th>bob_first</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Alice</th>\n",
       "      <td>2.68</td>\n",
       "      <td>3.31</td>\n",
       "      <td>3.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bob</th>\n",
       "      <td>3.91</td>\n",
       "      <td>3.56</td>\n",
       "      <td>2.87</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       alice_first  individual  bob_first\n",
       "Alice         2.68        3.31       3.48\n",
       "Bob           3.91        3.56       2.87"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "s3combined_list = []\n",
    "s3individual_list = []\n",
    "s3bobfirst_list = []\n",
    "\n",
    "for i in range(1000):\n",
    "    lst = [[0] * 8 for i in range(36)]\n",
    "    for i in range(len(lst)):\n",
    "        lst[i][np.random.randint(0, 8)] = 1\n",
    "    s3bob_q = np.array(lst)\n",
    "    s3alice_q = AllRange(8).dense_matrix()\n",
    "\n",
    "    # combined\n",
    "    s3combined = pmw2(workload=np.vstack((s3alice_q, s3bob_q)), x=x_small, eps=2, T=40, k=10,  \n",
    "                                 analyst_labels=['Alice'] * 36 + ['Bob'] * 36, \n",
    "                                 to_return='error', show_plot=False, show_messages=False, show_failure_step=False)\n",
    "    s3combined_list.append(s3combined)\n",
    "    \n",
    "    \n",
    "    s3bobfirst = pmw2(workload=np.vstack((s3bob_q, s3alice_q)), x=x_small, eps=2, T=40, k=10,  \n",
    "                                 analyst_labels=['Bob'] * 36 + ['Alice'] * 36, \n",
    "                                 to_return='error', show_plot=False, show_messages=False, show_failure_step=False)\n",
    "    s3bobfirst_list.append(s3bobfirst)\n",
    "    \n",
    "    # individual\n",
    "    s3a = pmw2(workload=s3alice_q, x=x_small, eps=1, T=40, k=5,\n",
    "               analyst_labels=['Alice'] * 36, \n",
    "               to_return='error', show_plot=False, show_messages=False, show_failure_step=False)\n",
    "    s3b = pmw2(workload=s3bob_q, x=x_small, eps=1, T=40, k=5,\n",
    "               analyst_labels=['Bob'] * 36, \n",
    "               to_return='error', show_plot=False, show_messages=False, show_failure_step=False)\n",
    "    s3individual = {**s3a, **s3b}\n",
    "    s3individual_list.append(s3individual)\n",
    "    \n",
    "# find mean over multiple trials\n",
    "s3combined_average = dict(pd.DataFrame(s3combined_list).mean())\n",
    "s3individual_average = dict(pd.DataFrame(s3individual_list).mean())\n",
    "s3bobfirst_average = dict(pd.DataFrame(s3bobfirst_list).mean())\n",
    "\n",
    "d = {'alice_first': s3combined_average, 'individual': s3individual_average, 'bob_first': s3bobfirst_average}\n",
    "df = pd.DataFrame(data=d).sort_index()\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "601b2e89",
   "metadata": {},
   "source": [
    "Alice = All Range\n",
    "\n",
    "Bob = Singleton\n",
    "\n",
    "In All Range -> Singleton case, 1.22 more error for second person\n",
    "\n",
    "In Singleton -> All Range case, 0.62 more error for second person\n",
    "\n",
    "This makes sense to me because the All Range workload includes the Singleton queries, so the synthetic database had already been updated to respond to those types of queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ff21a6af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average error is 0.08705555555555555. Lower bound is 0.0 and upper bound is 0.238\n"
     ]
    }
   ],
   "source": [
    "printBoundsAndAvgError(pmw2(workload=np.vstack((s3alice_q, s3bob_q)), x=x_small, eps=2, T=40, k=10,  \n",
    "                                 analyst_labels=['Alice'] * 36 + ['Bob'] * 36, \n",
    "                                 to_return='pd', show_plot=False, show_messages=False, show_failure_step=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f832dd9c",
   "metadata": {},
   "source": [
    "# experiment 4: exact same workload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "973dc5c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alice_first</th>\n",
       "      <th>individual</th>\n",
       "      <th>bob_first</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Alice</th>\n",
       "      <td>1.92</td>\n",
       "      <td>2.83</td>\n",
       "      <td>3.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bob</th>\n",
       "      <td>3.06</td>\n",
       "      <td>2.83</td>\n",
       "      <td>1.90</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       alice_first  individual  bob_first\n",
       "Alice         1.92        2.83       3.06\n",
       "Bob           3.06        2.83       1.90"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "s4combined_list = []\n",
    "s4individual_list = []\n",
    "s4bobfirst_list = []\n",
    "\n",
    "for i in range(1000):\n",
    "    s4alice_q = np.random.randint(2, size=(25,8))\n",
    "    s4bob_q = s4alice_q\n",
    "\n",
    "    # combined\n",
    "    s4combined = pmw2(workload=np.vstack((s4alice_q, s4bob_q)), x=x_small, eps=2, T=40, k=10,  \n",
    "                                 analyst_labels=['Alice'] * 25 + ['Bob'] * 25, \n",
    "                                 to_return='error', show_plot=False, show_messages=False, show_failure_step=False)\n",
    "    s4combined_list.append(s4combined)\n",
    "    \n",
    "    s4bobfirst = pmw2(workload=np.vstack((s4bob_q, s4alice_q)), x=x_small, eps=2, T=40, k=10,  \n",
    "                                 analyst_labels=['Bob'] * 25 + ['Alice'] * 25, \n",
    "                                 to_return='error', show_plot=False, show_messages=False, show_failure_step=False)\n",
    "    s4bobfirst_list.append(s4bobfirst)\n",
    "    \n",
    "    # individual\n",
    "    s4a = pmw2(workload=s4alice_q, x=x_small, eps=1, T=40, k=5,\n",
    "               analyst_labels=['Alice'] * 25, \n",
    "               to_return='error', show_plot=False, show_messages=False, show_failure_step=False)\n",
    "    s4b = pmw2(workload=s4bob_q, x=x_small, eps=1, T=40, k=5,\n",
    "               analyst_labels=['Bob'] * 25, \n",
    "               to_return='error', show_plot=False, show_messages=False, show_failure_step=False)\n",
    "    s4individual = {**s4a, **s4b}\n",
    "    s4individual_list.append(s4individual)\n",
    "    \n",
    "# find mean over multiple trials\n",
    "s4combined_average = dict(pd.DataFrame(s4combined_list).mean())\n",
    "s4individual_average = dict(pd.DataFrame(s4individual_list).mean())\n",
    "s4bobfirst_average = dict(pd.DataFrame(s4bobfirst_list).mean())\n",
    "\n",
    "d = {'alice_first': s4combined_average, 'individual': s4individual_average, 'bob_first': s4bobfirst_average}\n",
    "df = pd.DataFrame(data=d).sort_index()\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "395753a8",
   "metadata": {},
   "source": [
    "Makes sense to me. We are just switching the orders in which Alice and Bob are querying, and they have the exact same queries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "594131a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average error is 0.08650000000000002. Lower bound is 0.002 and upper bound is 0.28\n"
     ]
    }
   ],
   "source": [
    "printBoundsAndAvgError(pmw2(workload=np.vstack((s4alice_q, s4bob_q)), x=x_small, eps=2, T=40, k=10,  \n",
    "                                 analyst_labels=['Alice'] * 25 + ['Bob'] * 25, \n",
    "                                 to_return='pd', show_plot=False, show_messages=False, show_failure_step=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12bb6d0e",
   "metadata": {},
   "source": [
    "# To Do's (10/12)\n",
    "1. [DONE] Do 1000 Trials for scenario 2 and 3\n",
    "2. [DONE] Explaining the scenarios (why we chose these scenarios)\n",
    "- Output the 3 tables. Practice explaining the tables to someone who doesn't know what's going on. \n",
    "- The general idea is that we want to show that the problem exist. These experiments show that standard online answering algorithms still have problems. \n",
    "    - experiment 1: disjoint, most adversarial setting. \n",
    "    - experiment 2: if we make a less extreme setting, alice only cares about most of the dataset, we still have this problem \n",
    "    - experiment 3: even when we use the entire database and use different types of database, we still run into this problem. Bob's workload is even embedded in Alice's workload!!\n",
    "    - experiment 4: give them the exact same things, and bob is stil worse off. \n",
    "        - opposite of the free-rider problem - some folks can freely benefit from the public road without paying for it. in this case, if Bob had better error than Alice, then it would be free-rider problem. Not only are update steps better for making ur synthetic database better, update steps are better than any synthetic answer at all\n",
    "    \n",
    "    \n",
    "    \n",
    "- the multiplicative update step (non-private) = no regret learning - suppose there are true weights you give to weather experts who are guessing the weather - everytime you make an update step, the relative entropy between the weights you have and the true weights should go down. This is an average case guarantee. the problem with fairness topics is that average means nothing. \n",
    "- after tuesday (next friday) - implement PMW where each analyst is given some fraction of the total update stpes and see if the problem still remains. different from individual setting because they'd be sharing the same synthetic database."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174e2a24",
   "metadata": {},
   "source": [
    "# TO DO'S (10/17)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae1fcb55",
   "metadata": {},
   "source": [
    "- [DONE] Do experiments with Bob first, see if Alice faces the same error. \n",
    "- [DONE] What is the average error of a query?\n",
    "- [DONE] What is the query with the least and most error? \n",
    "    - To figure out what things look like for each analyst and an upper/lower bound on error\n",
    "- Write code to put the functions into production for multiple tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "708b108c",
   "metadata": {},
   "source": [
    "# To Do's (10/18)\n",
    "\n",
    "## Conclusions...\n",
    "We did a sanity check by reversing the order of Alice and Bob. \n",
    "\n",
    "There are mechanisms that are very specific to types of queries. H-trees are good for range queries. PMW could be the case where that PMW is good for some class of queries. We've proved that it's not\n",
    "\n",
    "## Nextâ€¦\n",
    "\n",
    "Update steps matter, update steps matter but end synthetic database is also valuable. \n",
    "\n",
    "- If only the update steps matter, then if we expand these query sets to very long, they should have relatively same error as doing over the entire sequence. \n",
    "\n",
    "- If the final synthetic database is also valuable, then there should be a noticable difference \n",
    "\n",
    "try to make an adaptation where once you make your experiment, for some large number of timestamps, randomly choose a query \n",
    "\n",
    "25/25 normal, for next 950 query, pick one on random and ask it repeatedly\n",
    "\n",
    "we want to see the average error from queries as time goes on. we will dilute the effect of the update steps. you should know which analysts it's from. \n",
    "\n",
    "Alice asks queries, bob asks queries. After they've asked their 25 each. Flip a coin. If heads, alice asks her queries at random. If tails, bob asks her queries at random. we want to see the effects of answering their queries on the synthetic database. In the case where we ask incompatible workloads, we saw there was the difference. do this once for each for the four queries, loook at the average error. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "language": "python",
   "name": "python388jvsc74a57bd0718e77e574b71e9f7991c7da6831896cfd7281e366db0dbf84de44e8d5f66e5a"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
